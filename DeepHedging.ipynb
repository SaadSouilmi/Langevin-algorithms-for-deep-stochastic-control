{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from optimizers.lrmsprop import LRMSprop\n",
    "from optimizers.ladam import LAdam\n",
    "from optimizers.ladadelta import LAdadelta\n",
    "from optimizers.lsgd import APLSGD\n",
    "from optimizers.ladagrad import LAdagrad\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scienceplots\n",
    "from training.train import train, plot_langevin_loss\n",
    "from diffusion.deephedging import DeepHedging\n",
    "from scheduler import PiecewiseConstantScheduler\n",
    "import os\n",
    "\n",
    "plt.style.use([\"science\", \"no-latex\", \"grid\"])\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we attempt to replicate a $\\mathcal F_T\\text{-measurable}$ payoff $Z$ defined on some portfolio $S_t\\in\\mathbb R^{d_1}$. The control $u_t\\in\\mathbb R^{d_1}$ represents the amount held at time $t$ of each asset. The objective is :\n",
    "$$ J(u, w) := \\mathbb E\\left[w + l\\left(Z - \\sum_{k=0}^{N-1} \\left\\langle u_{t_k}, S_{t_{k+1}} - S_{t_k}\\right\\rangle + \\sum_{k=0}^{N-1} \\left\\langle c_{t_r}S_{t_k}, \\left|u_{t_k} - u_{t_{k-1}}\\right|\\right\\rangle - w \\right)\\right]$$\n",
    "\n",
    "Where $l: \\mathbb R\\to \\mathbb R$ is a continuous, non-decreasing convex function. In our experiments we will consider the loss function associated with the value-at-risk i.e: $l: x\\mapsto (1-\\alpha)^{-1}\\max(x, 0)$. We will also work in the case where $d_1$ is even, such that for $d'_1 = d_1 / 2$ the assets $(S_t^{1, i})_{1\\leq i \\leq d'_1} = (S_t^i)_{1\\leq i \\leq d'_1}$ follow $d'_1$ independant Heston dynamics, and \n",
    "$(S_t^{2, i})_{1\\leq i \\leq d'_1} = (S_t^i)_{d'_1 +1\\leq i \\leq d_1}$ are the corresponding variance swaps, i.e:\n",
    "$$\\left\\{\\begin{align}\n",
    "    &dS_t^{1, i} = \\sqrt{V_t^i} S_t^{1, i} \\,dB^i_t\\;,\\;\\; S_0^{1,i} = s_0^i\\\\\n",
    "    & dV_t^i = a^i(b^i - V_t^i)dt + \\eta^i\\sqrt{V_t^i}\\,dW^i_t\\;,\\;\\;V_0^i = v_0^i\\\\\n",
    "    & d\\left\\langle B^i, W^i\\right\\rangle_t = \\rho^i\n",
    "\\end{align} \\right.$$\n",
    "and :\n",
    "$$\\begin{align}\n",
    "    &S_t^{2, i} := \\mathbb E\\left[\\int_0^T V_s^i\\,ds\\Big | \\mathcal F_t\\right] = \\int_0^t V_s^i\\,ds + L^i(t, V_t^i) \\\\\n",
    "    & L^i(t, v) := \\frac{v - b^i}{a^i}\\left(1 - e^{- a^i(T-t)}\\right) + b^i(T-t)\n",
    "\\end{align}$$\n",
    "The payoff is chosen to be call options over the tradable assets $S^1$ i.e: $Z = \\sum_{i=1} ^{d'_1} \\left(S_T^{1, i} - K^i\\right)_+$.\n",
    "For the parameters:\n",
    "$$\\begin{align}\n",
    "& d'_1 = 5, \\; T=1, \\; a=1_{\\mathbb R^5}, \\;b=0.04 \\times 1_{\\mathbb R^5}, \\; \\eta = 2\\times 1_{\\mathbb R^5}, \\; \\rho = -0.7 \\times 1_{\\mathbb R^5}, \\; \\alpha = 0.9, \\\\\n",
    "&s_0 = K = 1_{\\mathbb R^5}, \\; v_0 = 0.1 \\times 1_{\\mathbb R^5}, \\; c_{t_r} = 5e-4\\times 1_{\\mathbb R^5}.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = 512\n",
    "test_batch = 512\n",
    "train_size = 5\n",
    "test_size = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_diag_single_ctrl(\n",
    "    model_config,\n",
    "    N_euler,\n",
    "    control_config,\n",
    "    optim,\n",
    "    langevin_optim,\n",
    "    name,\n",
    "    lr,\n",
    "    target_lr,\n",
    "    sigma,\n",
    "    epochs,\n",
    "    total_iters,\n",
    "    **optim_kwargs,\n",
    "):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
    "\n",
    "    for i, N in enumerate(N_euler):\n",
    "        # Vanilla algo\n",
    "        ocp = DeepHedging(**model_config, N_euler=N)\n",
    "        ocp.set_control(control_config, multiple_controls=False)\n",
    "        optim_w = torch.optim.SGD(ocp.w.parameters(), lr=2e-3, momentum=0.9)\n",
    "        vanilla_optim = optim(ocp.control.parameters(), lr=lr, **optim_kwargs)\n",
    "        scheduler = PiecewiseConstantScheduler(\n",
    "            vanilla_optim, target_lr=target_lr, target_sigma=0, total_iters=total_iters\n",
    "        )\n",
    "        scheduler_w = PiecewiseConstantScheduler(\n",
    "            optim_w, target_lr=target_lr, target_sigma=0, total_iters=total_iters\n",
    "        )\n",
    "        train_loss, test_loss, test_ci = train(\n",
    "            ocp,\n",
    "            [vanilla_optim, optim_w],\n",
    "            [scheduler, scheduler_w],\n",
    "            name,\n",
    "            epochs,\n",
    "            train_size,\n",
    "            test_size,\n",
    "            train_batch,\n",
    "            test_batch,\n",
    "        )\n",
    "        axs[i].plot(\n",
    "            np.arange(len(test_loss)),\n",
    "            test_loss,\n",
    "            marker=\"o\",\n",
    "            mec=\"k\",\n",
    "            ms=3,\n",
    "            label=name,\n",
    "        )\n",
    "        axs[i].fill_between(\n",
    "            np.arange(len(test_loss)),\n",
    "            np.array(test_loss) - np.array(test_ci),\n",
    "            np.array(test_loss) + np.array(test_ci),\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "        # Langevin algo\n",
    "        ocp = DeepHedging(**model_config, N_euler=N)\n",
    "        ocp.set_control(control_config, multiple_controls=False)\n",
    "        optim_w = torch.optim.SGD(ocp.w.parameters(), lr=2e-3, momentum=0.9)\n",
    "        loptim = langevin_optim(ocp.control.parameters(), lr=lr, sigma=sigma[i])\n",
    "        scheduler = PiecewiseConstantScheduler(\n",
    "            loptim, target_lr=target_lr, target_sigma=0, total_iters=total_iters\n",
    "        )\n",
    "        scheduler_w = PiecewiseConstantScheduler(\n",
    "            optim_w, target_lr=target_lr, target_sigma=0, total_iters=total_iters\n",
    "        )\n",
    "        train_loss, test_loss, test_ci = train(\n",
    "            ocp,\n",
    "            [loptim, optim_w],\n",
    "            [scheduler, scheduler_w],\n",
    "            f\"L-{name}\",\n",
    "            epochs,\n",
    "            train_size,\n",
    "            test_size,\n",
    "            train_batch,\n",
    "            test_batch,\n",
    "        )\n",
    "        axs[i].plot(\n",
    "            np.arange(len(test_loss)),\n",
    "            test_loss,\n",
    "            marker=\"o\",\n",
    "            mec=\"k\",\n",
    "            ms=3,\n",
    "            label=f\"L-{name}\",\n",
    "        )\n",
    "        axs[i].fill_between(\n",
    "            np.arange(len(test_loss)),\n",
    "            np.array(test_loss) - np.array(test_ci),\n",
    "            np.array(test_loss) + np.array(test_ci),\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(r\"$J(u_\\theta)$\")\n",
    "        axs[i].set_xlabel(\"Epochs\")\n",
    "        if i == 2:\n",
    "            legend = axs[i].legend(fancybox=True, edgecolor=\"k\", loc=0)\n",
    "            legend.get_frame().set_linewidth(0.5)\n",
    "\n",
    "        axs[i].set_ylim((0, 1.2))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "alpha = 0.9\n",
    "ell = lambda x: nn.functional.relu(x) / (1.0 - alpha)\n",
    "dim = 5\n",
    "s0 = 1.0\n",
    "v0 = 0.1\n",
    "\n",
    "\n",
    "model_config = dict(\n",
    "    T=1.0,\n",
    "    dim=dim,\n",
    "    ell=ell,\n",
    "    a=torch.ones(dim).to(device),\n",
    "    b=0.04 * torch.ones(dim).to(device),\n",
    "    sigma=2.0 * torch.ones(dim).to(device),\n",
    "    rho=-0.7 * torch.ones(dim).to(device),\n",
    "    K=s0 * torch.ones(dim).to(device),\n",
    "    T_COST=5e-4,\n",
    "    S_0=s0 * torch.ones(dim).to(device),\n",
    "    V_0=v0 * torch.ones(dim).to(device),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "control_config = dict(\n",
    "    input_dim=1 + 4 * dim,\n",
    "    output_dim=2 * dim,\n",
    "    hidden_dim=32,\n",
    "    depth=2,\n",
    "    activation=nn.ReLU(),\n",
    "    out_transform=nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_single_ctrl = test_diag_single_ctrl(\n",
    "    model_config,\n",
    "    [30, 50, 100],\n",
    "    control_config,\n",
    "    torch.optim.Adam,\n",
    "    LAdam,\n",
    "    \"Adam\",\n",
    "    2e-3,\n",
    "    2e-4,\n",
    "    [2e-4, 2e-4, 2e-4],\n",
    "    100,\n",
    "    80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad_single_ctrl = test_diag_single_ctrl(\n",
    "    model_config,\n",
    "    [30, 50, 100],\n",
    "    control_config,\n",
    "    torch.optim.Adagrad,\n",
    "    LAdagrad,\n",
    "    \"Adagrad\",\n",
    "    7e-3,\n",
    "    7e-4,\n",
    "    [1e-5, 1e-5, 1e-5],\n",
    "    100,\n",
    "    80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adadelta_single_ctrl = test_diag_single_ctrl(\n",
    "    model_config,\n",
    "    [30, 50, 100],\n",
    "    control_config,\n",
    "    torch.optim.Adadelta,\n",
    "    LAdadelta,\n",
    "    \"Adadelta\",\n",
    "    5e-1,\n",
    "    5e-2,\n",
    "    [5e-3, 5e-3, 5e-3],\n",
    "    100,\n",
    "    80,\n",
    "    rho=0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop_single_ctrl = test_diag_single_ctrl(\n",
    "    model_config,\n",
    "    [30, 50, 100],\n",
    "    control_config,\n",
    "    torch.optim.RMSprop,\n",
    "    LRMSprop,\n",
    "    \"RMSprop\",\n",
    "    2e-3,\n",
    "    2e-4,\n",
    "    [2e-4, 2e-4, 2e-4],\n",
    "    100,\n",
    "    80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_single_ctrl = test_diag_single_ctrl(\n",
    "    model_config,\n",
    "    [30, 50, 100],\n",
    "    control_config,\n",
    "    torch.optim.SGD,\n",
    "    APLSGD,\n",
    "    \"SGD\",\n",
    "    3e-2,\n",
    "    3e-3,\n",
    "    [5e-5, 5e-5, 5e-5],\n",
    "    100,\n",
    "    80,\n",
    "    momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"graphs/deephedging\"):\n",
    "    adam_single_ctrl.savefig(\"graphs/deephedging/adam_single_ctrl.pdf\")\n",
    "    adadelta_single_ctrl.savefig(\"graphs/deephedging/adadelta_single_ctrl.pdf\")\n",
    "    rmsprop_single_ctrl.savefig(\"graphs/deephedging/rmsprop_single_ctrl.pdf\")\n",
    "    sgd_single_ctrl.savefig(\"graphs/deephedging/sgd_single_ctrl.pdf\")\n",
    "    adagrad_single_ctrl.savefig(\"graphs/deephedging/adagrad_single_ctrl.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_diag_multiple_ctrls(\n",
    "    model_config,\n",
    "    N_euler,\n",
    "    control_config,\n",
    "    optim,\n",
    "    langevin_optim,\n",
    "    name,\n",
    "    lr,\n",
    "    target_lr,\n",
    "    sigma,\n",
    "    ll,\n",
    "    epochs,\n",
    "    total_iters,\n",
    "    **optim_kwargs,\n",
    "):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
    "\n",
    "    for i, N in enumerate(N_euler):\n",
    "        # Vanilla algo\n",
    "        ocp = DeepHedging(**model_config, N_euler=N)\n",
    "        ocp.set_control(control_config, multiple_controls=True)\n",
    "        optim_w = torch.optim.SGD(ocp.w.parameters(), lr=2e-3, momentum=0.9)\n",
    "        vanilla_optim = [\n",
    "            optim(control.parameters(), lr=lr, **optim_kwargs)\n",
    "            for control in ocp.control\n",
    "        ]\n",
    "        scheduler_w = PiecewiseConstantScheduler(\n",
    "            optim_w, target_lr=target_lr, target_sigma=0, total_iters=total_iters\n",
    "        )\n",
    "        scheduler = [\n",
    "            PiecewiseConstantScheduler(\n",
    "                optimizer, target_lr=target_lr, target_sigma=0, total_iters=total_iters\n",
    "            )\n",
    "            for optimizer in vanilla_optim\n",
    "        ]\n",
    "        train_loss, test_loss, test_ci = train(\n",
    "            ocp,\n",
    "            [optim_w, *vanilla_optim],\n",
    "            [scheduler_w, *scheduler],\n",
    "            name,\n",
    "            epochs,\n",
    "            train_size,\n",
    "            test_size,\n",
    "            train_batch,\n",
    "            test_batch,\n",
    "        )\n",
    "        axs[i].plot(np.arange(len(test_loss)), test_loss, label=name)\n",
    "        axs[i].fill_between(\n",
    "            np.arange(len(test_loss)),\n",
    "            np.array(test_loss) - np.array(test_ci),\n",
    "            np.array(test_loss) + np.array(test_ci),\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "        # Langevin algo\n",
    "        ocp = DeepHedging(**model_config, N_euler=N)\n",
    "        ocp.set_control(control_config, multiple_controls=True)\n",
    "        optim_w = torch.optim.SGD(ocp.w.parameters(), lr=2e-3, momentum=0.9)\n",
    "        loptim = [\n",
    "            langevin_optim(control.parameters(), lr=lr, sigma=sigma[i])\n",
    "            for control in ocp.control\n",
    "        ]\n",
    "        scheduler_w = PiecewiseConstantScheduler(\n",
    "            optim_w, target_lr=target_lr, target_sigma=0, total_iters=total_iters\n",
    "        )\n",
    "        scheduler = [\n",
    "            PiecewiseConstantScheduler(\n",
    "                optimizer, target_lr=target_lr, target_sigma=0, total_iters=total_iters\n",
    "            )\n",
    "            for optimizer in loptim\n",
    "        ]\n",
    "        train_loss, test_loss, test_ci = train(\n",
    "            ocp,\n",
    "            [optim_w, *loptim],\n",
    "            [scheduler_w, *scheduler],\n",
    "            f\"L-{name}\",\n",
    "            epochs,\n",
    "            train_size,\n",
    "            test_size,\n",
    "            train_batch,\n",
    "            test_batch,\n",
    "        )\n",
    "        axs[i].plot(\n",
    "            np.arange(len(test_loss)),\n",
    "            test_loss,\n",
    "            label=f\"L-{name}\",\n",
    "        )\n",
    "        axs[i].fill_between(\n",
    "            np.arange(len(test_loss)),\n",
    "            np.array(test_loss) - np.array(test_ci),\n",
    "            np.array(test_loss) + np.array(test_ci),\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "        # Layer Langevin algo\n",
    "        for ll_rate in ll:\n",
    "            ocp = DeepHedging(**model_config, N_euler=N)\n",
    "            ocp.set_control(control_config, multiple_controls=True)\n",
    "            optim_w = torch.optim.SGD(ocp.w.parameters(), lr=2e-3, momentum=0.9)\n",
    "            scheduler_w = PiecewiseConstantScheduler(\n",
    "                optim_w, target_lr=target_lr, target_sigma=0, total_iters=total_iters\n",
    "            )\n",
    "            lloptim = []\n",
    "            scheduler = []\n",
    "            for j, control in enumerate(ocp.control):\n",
    "                if 100 * j / len(ocp.control) <= ll_rate:\n",
    "                    lloptim.append(\n",
    "                        langevin_optim(control.parameters(), lr=lr, sigma=sigma[i])\n",
    "                    )\n",
    "                else:\n",
    "                    lloptim.append(optim(control.parameters(), lr=lr, **optim_kwargs))\n",
    "                scheduler.append(\n",
    "                    PiecewiseConstantScheduler(\n",
    "                        lloptim[j],\n",
    "                        target_lr=target_lr,\n",
    "                        target_sigma=0,\n",
    "                        total_iters=total_iters,\n",
    "                    )\n",
    "                )\n",
    "            train_loss, test_loss, test_ci = train(\n",
    "                ocp,\n",
    "                [optim_w, *lloptim],\n",
    "                [scheduler_w, *scheduler],\n",
    "                f\"LL-{name} {ll_rate}%\",\n",
    "                epochs,\n",
    "                train_size,\n",
    "                test_size,\n",
    "                train_batch,\n",
    "                test_batch,\n",
    "            )\n",
    "            axs[i].plot(\n",
    "                np.arange(len(test_loss)),\n",
    "                test_loss,\n",
    "                label=f\"LL-{name} {ll_rate}%\",\n",
    "            )\n",
    "            axs[i].fill_between(\n",
    "                np.arange(len(test_loss)),\n",
    "                np.array(test_loss) - np.array(test_ci),\n",
    "                np.array(test_loss) + np.array(test_ci),\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(r\"$J(u_\\theta)$\")\n",
    "        axs[i].set_xlabel(\"Epochs\")\n",
    "        if i == 2:\n",
    "            legend = axs[i].legend(fancybox=True, edgecolor=\"k\", loc=0)\n",
    "            legend.get_frame().set_linewidth(0.5)\n",
    "\n",
    "        axs[i].set_ylim((0.35, 1.2))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    T=1.0,\n",
    "    dim=dim,\n",
    "    ell=ell,\n",
    "    a=torch.ones(dim).to(device),\n",
    "    b=0.04 * torch.ones(dim).to(device),\n",
    "    sigma=2.0 * torch.ones(dim).to(device),\n",
    "    rho=-0.7 * torch.ones(dim).to(device),\n",
    "    K=s0 * torch.ones(dim).to(device),\n",
    "    T_COST=5e-4,\n",
    "    S_0=s0 * torch.ones(dim).to(device),\n",
    "    V_0=v0 * torch.ones(dim).to(device),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "control_config = dict(\n",
    "    input_dim=4 * dim,\n",
    "    output_dim=2 * dim,\n",
    "    hidden_dim=32,\n",
    "    depth=2,\n",
    "    activation=nn.ReLU(),\n",
    "    out_transform=nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_multiple_ctrl = test_diag_multiple_ctrls(\n",
    "    model_config,\n",
    "    [10, 20, 40],\n",
    "    control_config,\n",
    "    optim.Adam,\n",
    "    LAdam,\n",
    "    \"Adam\",\n",
    "    2e-3,\n",
    "    2e-4,\n",
    "    [2e-4, 2e-4, 2e-4],\n",
    "    [30, 90],\n",
    "    200,\n",
    "    180,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad_multiple_ctrl = test_diag_multiple_ctrls(\n",
    "    model_config,\n",
    "    [10, 20, 40],\n",
    "    control_config,\n",
    "    optim.Adagrad,\n",
    "    LAdagrad,\n",
    "    \"Adagrad\",\n",
    "    7e-3,\n",
    "    7e-4,\n",
    "    [1e-5, 1e-5, 1e-5],\n",
    "    [30, 90],\n",
    "    200,\n",
    "    180,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adadelta_multiple_ctrl = test_diag_multiple_ctrls(\n",
    "    model_config,\n",
    "    [10, 20, 40],\n",
    "    control_config,\n",
    "    optim.Adadelta,\n",
    "    LAdadelta,\n",
    "    \"Adadelta\",\n",
    "    5e-1,\n",
    "    5e-2,\n",
    "    [5e-3, 5e-3, 5e-3],\n",
    "    [30, 90],\n",
    "    200,\n",
    "    180,\n",
    "    rho=0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop_multiple_ctrl = test_diag_multiple_ctrls(\n",
    "    model_config,\n",
    "    [10, 20, 40],\n",
    "    control_config,\n",
    "    optim.RMSprop,\n",
    "    LRMSprop,\n",
    "    \"RMSprop\",\n",
    "    2e-3,\n",
    "    2e-4,\n",
    "    [2e-3, 2e-3, 2e-3],\n",
    "    [30, 90],\n",
    "    200,\n",
    "    180,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_multiple_ctrl = test_diag_multiple_ctrls(\n",
    "    model_config,\n",
    "    [10, 20, 40],\n",
    "    control_config,\n",
    "    optim.SGD,\n",
    "    APLSGD,\n",
    "    \"SGD\",\n",
    "    5e-2,\n",
    "    5e-3,\n",
    "    [2e-3, 2e-3, 2e-3],\n",
    "    [30, 90],\n",
    "    200,\n",
    "    180,\n",
    "    momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"graphs/deephedging\"):\n",
    "    adam_multiple_ctrl.savefig(\"graphs/deephedging/adam_multiple_ctrl.pdf\")\n",
    "    adadelta_multiple_ctrl.savefig(\"graphs/deephedging/adadelta_multiple_ctrl.pdf\")\n",
    "    rmsprop_multiple_ctrl.savefig(\"graphs/deephedging/rmsprop_multiple_ctrl.pdf\")\n",
    "    sgd_multiple_ctrl.savefig(\"graphs/deephedging/sgd_multiple_ctrl.pdf\")\n",
    "    adagrad_multiple_ctrl.savefig(\"graphs/deephedging/adagrad_multiple_ctrl.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9\n",
    "ell = lambda x: nn.functional.relu(x) / (1.0 - alpha)\n",
    "dim = 1\n",
    "s0 = 1.0\n",
    "v0 = 0.1\n",
    "\n",
    "\n",
    "model_config = dict(\n",
    "    T=1.0,\n",
    "    N_euler=50,\n",
    "    dim=dim,\n",
    "    ell=ell,\n",
    "    a=torch.ones(dim),\n",
    "    b=0.04 * torch.ones(dim),\n",
    "    sigma=2.0 * torch.ones(dim),\n",
    "    rho=-0.7 * torch.ones(dim),\n",
    "    K=s0 * torch.ones(dim),\n",
    "    T_COST=5e-4,\n",
    "    S_0=s0 * torch.ones(dim),\n",
    "    V_0=v0 * torch.ones(dim),\n",
    ")\n",
    "\n",
    "control_config = dict(\n",
    "    input_dim=1 + 4 * dim,\n",
    "    output_dim=2 * dim,\n",
    "    hidden_dim=32,\n",
    "    depth=2,\n",
    "    activation=nn.ReLU(),\n",
    "    out_transform=nn.ReLU(),\n",
    ")\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocp = DeepHedging(**model_config)\n",
    "ocp.set_control(control_config, multiple_controls=False)\n",
    "optim_w = optim.SGD(ocp.w.parameters(), lr=2e-3, momentum=0.9)\n",
    "adam_control = optim.Adam(ocp.control.parameters(), lr=2e-3)\n",
    "adam = [optim_w, adam_control]\n",
    "\n",
    "scheduler_w = PiecewiseConstantScheduler(\n",
    "    optim_w, target_lr=2e-4, target_sigma=0, total_iters=80\n",
    ")\n",
    "scheduler_control = PiecewiseConstantScheduler(\n",
    "    adam_control, target_lr=2e-4, target_sigma=0, total_iters=80\n",
    ")\n",
    "scheduler = [scheduler_w, scheduler_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_adam, test_loss_adam, test_ci_adam = train(\n",
    "    ocp,\n",
    "    adam,\n",
    "    scheduler,\n",
    "    \"Adam\",\n",
    "    epochs,\n",
    "    train_size,\n",
    "    test_size,\n",
    "    train_batch,\n",
    "    test_batch,\n",
    "    False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1, S2, V, u = ocp.sample_traj(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "t = model_config[\"T\"] / model_config[\"N_euler\"] * np.arange(model_config[\"N_euler\"] + 1)\n",
    "plt.plot(\n",
    "    t,\n",
    "    S1.flatten().detach().numpy(),\n",
    "    label=r\"$S_t^1$\",\n",
    "    marker=\"s\",\n",
    "    mec=\"k\",\n",
    "    ms=3,\n",
    "    lw=1.5,\n",
    ")\n",
    "plt.plot(\n",
    "    t, V.flatten().detach().numpy(), label=r\"$V_t$\", marker=\"s\", mec=\"k\", ms=3, lw=1.5\n",
    ")\n",
    "plt.plot(\n",
    "    t,\n",
    "    u[:, :, 0].flatten().detach().numpy(),\n",
    "    label=r\"$u_t^1$\",\n",
    "    marker=\"^\",\n",
    "    mec=\"k\",\n",
    "    ms=3,\n",
    "    lw=1.5,\n",
    ")\n",
    "plt.plot(\n",
    "    t,\n",
    "    u[:, :, 1].flatten().detach().numpy(),\n",
    "    label=r\"$u_t^2$\",\n",
    "    marker=\"^\",\n",
    "    mec=\"k\",\n",
    "    ms=3,\n",
    "    lw=1.5,\n",
    ")\n",
    "\n",
    "legend = plt.legend(fancybox=True, edgecolor=\"black\", loc=0)\n",
    "legend.get_frame().set_linewidth(0.5)\n",
    "plt.title(\"Sample Trajectory\")\n",
    "plt.xlabel(r\"$t$\")\n",
    "# fig.savefig(\"graphs/deephedging/Sample_traj_hedging_N50.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9\n",
    "ell = lambda x: nn.functional.relu(x) / (1.0 - alpha)\n",
    "dim = 5\n",
    "s0 = 1.0\n",
    "v0 = 0.1\n",
    "\n",
    "\n",
    "model_config = dict(\n",
    "    T=1.0,\n",
    "    dim=dim,\n",
    "    ell=ell,\n",
    "    a=torch.ones(dim).to(device),\n",
    "    b=0.04 * torch.ones(dim).to(device),\n",
    "    sigma=2.0 * torch.ones(dim).to(device),\n",
    "    rho=-0.7 * torch.ones(dim).to(device),\n",
    "    K=s0 * torch.ones(dim).to(device),\n",
    "    T_COST=5e-4,\n",
    "    S_0=s0 * torch.ones(dim).to(device),\n",
    "    V_0=v0 * torch.ones(dim).to(device),\n",
    ")\n",
    "\n",
    "control_config = dict(\n",
    "    input_dim=1 + 4 * dim,\n",
    "    output_dim=2 * dim,\n",
    "    hidden_dim=32,\n",
    "    depth=4,\n",
    "    activation=nn.SiLU(),\n",
    "    out_transform=nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_single_ctrl = test_diag_single_ctrl(\n",
    "    model_config,\n",
    "    [30, 50, 100],\n",
    "    control_config,\n",
    "    torch.optim.Adam,\n",
    "    LAdam,\n",
    "    \"Adam\",\n",
    "    2e-3,\n",
    "    2e-4,\n",
    "    [2e-5, 2e-5, 2e-5],\n",
    "    100,\n",
    "    80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
